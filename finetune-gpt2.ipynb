{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7f771-c229-4d06-a184-61086c2f6e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade jupyter\n",
    "!pip install --upgrade ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5114e05-6bc6-44dd-b635-7c0a4b960650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.18.0\n",
    "!pip install datasets==2.9.0\n",
    "!pip install pandas==1.4.1\n",
    "!pip install numpy==1.22.2\n",
    "!pip install wandb==0.13.9\n",
    "!pip install torch==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b148e229-3459-4f37-8174-4aed38af8685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import Trainer\n",
    "import transformers \n",
    "import numpy as np\n",
    "import datasets\n",
    "import logging\n",
    "import torch\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93451546-6fa0-4b90-aa5e-254ef117d24c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_EPOCHS = 2\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "EVAL_BATCH_SIZE = 4\n",
    "MAX_LEN = 512\n",
    "LOGGING_STEPS = 64\n",
    "SAVE_STEPS = 10240  # reduce it to a smaler value like 512 if you want to save checkpoints\n",
    "SAVE_TOTAL_LIMIT = 2\n",
    "\n",
    "BOS_TOKEN = '<|startoftext|>'\n",
    "EOS_TOKEN = '<|endoftext|>'\n",
    "PAD_TOKEN = '<|pad|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9779be0f-cb0f-43ac-93d2-cc9afd9e79b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login <USE YOUR WEIGHTS & BIASES API KEY HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03fcd9da-5462-4291-9451-a7533d63fda5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/how-to-train-faq-chatbot-from-scratch/02-finetune/01-finetune-custom-gpt2.ipynb'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.abspath('finetune-gpt2.ipynb')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf0f5b7f-bc68-4150-b01e-72b5394cbef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35bf10e8-ade5-429e-b644-25ee4e14eeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1428\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 159\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_dataset = datasets.load_from_disk('/tokenized-custom')\n",
    "reloaded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5269f0fb-ab56-412f-aad0-b6f8cdab4c07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Tokenizer: PreTrainedTokenizer(name_or_path='.././01-tokenize/vocab-custom', vocab_size=50257, model_max_len=512, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|pad|>'})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('/tokenize/vocab-custom', \n",
    "                                          bos_token=BOS_TOKEN,\n",
    "                                          eos_token=EOS_TOKEN, \n",
    "                                          pad_token=PAD_TOKEN, \n",
    "                                          lower=True,\n",
    "                                          return_tensors='pt')\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.model_max_length = MAX_LEN\n",
    "logger.info(f'Tokenizer: {tokenizer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784ce134-ad54-4559-b652-d6134f92c0c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('arun-shankar/GPT-2-covid-news-articles').cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90e2331e-c9ab-48d5-a57d-7640d210691a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_data_collator(batch):\n",
    "    # batch size for data collation = per_device_train_batch_size * number of GPUs\n",
    "    input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "    attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "    labels = torch.stack([example['labels'] for example in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57bc6c01-1d7d-404f-b443-74ef5ebbcba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir='./model/custom-finetuned', \n",
    "                                  overwrite_output_dir=True, \n",
    "                                  num_train_epochs=TRAIN_EPOCHS,  \n",
    "                                  optim='adamw_torch', \n",
    "                                  save_strategy='steps', \n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  per_device_train_batch_size=TRAIN_BATCH_SIZE, \n",
    "                                  per_device_eval_batch_size=EVAL_BATCH_SIZE, \n",
    "                                  warmup_steps=10, \n",
    "                                  weight_decay=0.1,\n",
    "                                  logging_steps=LOGGING_STEPS,\n",
    "                                  save_steps=SAVE_STEPS, \n",
    "                                  save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "                                  report_to='wandb',\n",
    "                                  logging_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4436dddd-ec65-46b1-8570-a7d4c558e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, \n",
    "                  args=training_args, \n",
    "                  train_dataset=reloaded_dataset['train'], \n",
    "                  eval_dataset=reloaded_dataset['validation'], \n",
    "                  data_collator=custom_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585f52fc-f336-4bca-9430-079fde2a3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34090b38-f771-4876-8472-5def79f3ebff",
   "metadata": {},
   "source": [
    "#### Save finetuned model to local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97879c57-d3e1-40a4-b1c9-5fd577b39318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./model/custom-finetuned\n",
      "Configuration saved in ./model/custom-finetuned/config.json\n",
      "Model weights saved in ./model/custom-finetuned/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./model/finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad93deb0-af54-4a9f-bba0-2707b1fba969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
